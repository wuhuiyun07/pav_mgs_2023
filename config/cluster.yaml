# cluster.yaml - cluster configuration file
__default__:
  account: loni_virus2023
  partition: workq
  time: 72:00:00
  ntasks: 1
  cpus-per-task: 1
### rule-specific resources
trimming:
  time: 01-00:00:00
mapping:
  time: 01-00:00:00
  cpus-per-task: 16
assembling:
  time: 02-00:00:00
  cpus-per-task: 32



snakemake
    -j 10 \
    --cluster-config cluster.yaml \
    --cluster "sbatch \
               --job-name={rule}-{wildcards} \
               -A {cluster.account} \
               -p {cluster.partition} \
               -t {cluster.time} \
               --mem-per-cpu={resources.mem_mb}MB \
               --ntasks {cluster.ntasks} \
               --cpus-per-task {cluster.cpus-per-task} \
               --output=log/hpc/jobs/{rule}/{rule}.{wildcards}.out"


# cluster:
#     mkdir -p log/hpc/jobs/{rule}/ &&
#     sbatch
#         --job-name={rule}-{wildcards}
#         --account={resources.account}
#         --partition={resources.partition}
#         --mail-user={resources.email}
#         --mail-type={resources.mail_on}
#         --nodes=1
#         --ntasks=1
#         --cpus-per-task={threads}
#         --mem-per-cpu={resources.mem_mb}MB
#         --time={resources.time}
#         --output=log/hpc/jobs/{rule}/{rule}.{wildcards}.out
# default-resources:
#   - account=loni_virus2023
#   - partition=workq
#   - mail_on=FAIL
#   - mem_mb=196000
#   - time="72:00:00"
# conda-frontend: "mamba"
# immediate-submit: False
# jobs: 4999
# keep-going: true
# latency-wait: 120
# max-jobs-per-second: 100
# max-status-checks-per-second: 100
# notemp: true
# printshellcmds: true
# scheduler: greedy
# use-conda: true
# verbose: true